{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "run_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "input_dir = os.path.join(os.getcwd(), \"inputs\")\n",
    "output_dir = os.path.join(os.getcwd(), \"outputs\")\n",
    "\n",
    "input_csv = os.path.join(input_dir, \"api_result.csv\")\n",
    "output_csv = os.path.join(output_dir, f\"quarterly_holdings_as_of_{run_date}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only 100 max call per month, loop below counts 50 calls\n",
    "# replace with your api key registered here https://sec-api.io/docs/n-port-data-api \n",
    "api_key = \"bc0ce7af2bbc192fbe8f9479428ad787856df154f8fa8ea0054d2fbe8e919835\" \n",
    "url = f\"https://api.sec-api.io/form-nport?token={api_key}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run code below monthly (API free limit refreshes every month), api call result saved to api_result.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed with status code 429\n"
     ]
    }
   ],
   "source": [
    "all_data = pd.DataFrame()\n",
    "\n",
    "# Loop to fetch data in chunks of 10 (max return per call) until we have 500 results\n",
    "for i in range(15):\n",
    "    query_payload = {\n",
    "        \"query\": 'genInfo.regCik:\"0000810893\"', \n",
    "        # These are the funds included under cik 0000810893:\n",
    "        # https://www.sec.gov/Archives/edgar/data/810893/0000810893-19-003090-index.htm\n",
    "        \"from\": i * 10, \n",
    "        \"size\": 10,  \n",
    "        \"sort\": [{\"filedAt\": {\"order\": \"desc\"}}] \n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=query_payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        df = pd.json_normalize(data['filings'])\n",
    "        all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\")\n",
    "        break\n",
    "\n",
    "df_clean = all_data[['invstOrSecs', 'filedAt']]\n",
    "df_clean = df_clean.explode('invstOrSecs')\n",
    "df_clean = pd.concat([df_clean.drop(['invstOrSecs'], axis=1), df_clean['invstOrSecs'].apply(pd.Series)], axis=1)\n",
    "df_clean.to_csv(input_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in saved local csv, clean data to extract TIPS valUSD saved to as_of_run_date csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.read_csv(input_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df_clean[(df_clean['issuerCat'] == 'UST') & (df_clean['title'].str.contains('TSY INFL', case=False))]\n",
    "filtered_df = filtered_df[['filedAt','title','valUSD']]\n",
    "filtered_df.loc[:, 'filedAt'] = pd.to_datetime(filtered_df['filedAt'], utc=True).dt.date\n",
    "filtered_df.loc[:, 'valUSD (MM)'] = (filtered_df['valUSD'] / 1_000_000).astype(float).round(2)\n",
    "\n",
    "pivot_table = filtered_df.pivot_table(index='title', columns='filedAt', values='valUSD (MM)', aggfunc='sum')\n",
    "pivot_table = pivot_table.reindex(sorted(pivot_table.columns, reverse=True), axis=1).reset_index()\n",
    "pivot_table['extracted_date'] = pivot_table['title'].str.extract(r'(\\d{2}/\\d{2})')[0]\n",
    "pivot_table['extracted_date'] = pd.to_datetime(pivot_table['extracted_date'], format='%m/%y', errors='coerce')\n",
    "pivot_table_sorted = pivot_table.sort_values(by='extracted_date').set_index('title').drop('extracted_date', axis=1)\n",
    "# drop oldest column as it could be incomplete due to query limit\n",
    "pivot_table_sorted = pivot_table_sorted.iloc[:, :-1] \n",
    "pivot_table_sorted.to_csv(output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store history while append data with new runs to merged_quarterly_holdings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [f for f in os.listdir(output_dir) if f.endswith('.csv')]\n",
    "csv_files.sort(key=lambda x: datetime.strptime(x.split('_as_of_')[-1].split('.')[0], '%Y-%m-%d'))\n",
    "\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(output_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    if merged_df.empty:\n",
    "        merged_df = df\n",
    "    else:\n",
    "        # Find columns that are in the new DataFrame but not in the merged DataFrame\n",
    "        new_columns = df.columns.difference(merged_df.columns)\n",
    "        merged_df = pd.concat([merged_df, df[new_columns]], axis=1)\n",
    "        \n",
    "merged_df = merged_df.set_index('title')\n",
    "merged_df = merged_df.reindex(sorted(merged_df.columns, reverse=True), axis=1).reset_index()\n",
    "merged_df.to_excel(os.path.join(os.getcwd(), 'merged_quarterly_holdings.xlsx'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
